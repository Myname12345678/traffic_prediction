{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full-connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:34:27.068747Z",
     "start_time": "2018-02-27T07:34:27.054601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import lab.setup\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "g_region_temporal = 5\n",
    "g_region_spatial  = 1\n",
    "g_start_date = '2016-03-03'\n",
    "g_end_date   = '2016-03-03'\n",
    "g_start_time = '{} 00:00:00'.format(g_start_date)\n",
    "g_end_time   = '{} 23:59:59'.format(g_end_date)\n",
    "\n",
    "DATA_PATH = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:34:27.832865Z",
     "start_time": "2018-02-27T07:34:27.828516Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load from prepared dataset generated by linear.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:34:28.319325Z",
     "start_time": "2018-02-27T07:34:28.046667Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_train_full = pd.read_csv('dataset/ds_filled_s1.csv', dtype={'link_ID':'uint64'}, low_memory=False)\n",
    "ds_train_full.head(1)\n",
    "\n",
    "class PandasDataset(data.Dataset):\n",
    "    def __init__(self, df, feature_columns):\n",
    "        self.df = df[feature_columns].astype('float32')\n",
    "        self.dataset = self.df.values\n",
    "        self.temporal_order = g_region_temporal\n",
    "        self.feature_size = self.temporal_order * len(feature_columns)\n",
    "        self.len = self.df.shape[0] - (self.temporal_order - 1) - 1\n",
    "        self.label_index = self.df.columns.tolist().index('travel_time')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_vec = self.dataset[idx:idx+self.temporal_order].reshape(self.feature_size)\n",
    "        label_vec   = self.dataset[idx+self.temporal_order][self.label_index:self.label_index+1]\n",
    "        \n",
    "        return {'feature': feature_vec, 'label': label_vec}\n",
    "\n",
    "def collate(batch):\n",
    "    \"Puts each data field into a tensor with outer dimension batch size\"\n",
    "    feature_batch = torch.stack([torch.from_numpy(f['feature']) for f in batch], 0)\n",
    "    label_batch = torch.stack([torch.from_numpy(f['label']) for f in batch], 0)\n",
    "    return {\n",
    "        'feature': feature_batch, \n",
    "        'label': label_batch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:35:26.087654Z",
     "start_time": "2018-02-27T07:34:28.673395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 75075\n",
      "valid set size: 19305\n",
      "=== epoch[1/100], loss: 0.385971, valid_loss: 0.330751 ===\n",
      "=== epoch[2/100], loss: 0.294439, valid_loss: 0.250065 ===\n",
      "=== epoch[3/100], loss: 0.302112, valid_loss: 0.234529 ===\n",
      "=== epoch[4/100], loss: 0.343960, valid_loss: 0.254090 ===\n",
      "=== epoch[5/100], loss: 0.292957, valid_loss: 0.238681 ===\n",
      "=== epoch[6/100], loss: 0.255936, valid_loss: 0.235917 ===\n",
      "=== epoch[7/100], loss: 0.228289, valid_loss: 0.309622 ===\n",
      "=== epoch[8/100], loss: 0.231741, valid_loss: 0.251485 ===\n",
      "=== epoch[9/100], loss: 0.242353, valid_loss: 0.202187 ===\n",
      "=== epoch[10/100], loss: 0.240744, valid_loss: 0.277689 ===\n",
      "=== epoch[11/100], loss: 0.227105, valid_loss: 0.198969 ===\n",
      "=== epoch[12/100], loss: 0.200147, valid_loss: 0.190875 ===\n",
      "=== epoch[13/100], loss: 0.196064, valid_loss: 0.187158 ===\n",
      "=== epoch[14/100], loss: 0.195344, valid_loss: 0.183831 ===\n",
      "=== epoch[15/100], loss: 0.196942, valid_loss: 0.187719 ===\n",
      "=== epoch[16/100], loss: 0.203650, valid_loss: 0.193430 ===\n",
      "=== epoch[17/100], loss: 0.193345, valid_loss: 0.185919 ===\n",
      "=== epoch[18/100], loss: 0.192698, valid_loss: 0.186622 ===\n",
      "=== epoch[19/100], loss: 0.192391, valid_loss: 0.180925 ===\n",
      "=== epoch[20/100], loss: 0.190656, valid_loss: 0.179009 ===\n",
      "=== epoch[21/100], loss: 0.188447, valid_loss: 0.176081 ===\n",
      "=== epoch[22/100], loss: 0.187806, valid_loss: 0.176374 ===\n",
      "=== epoch[23/100], loss: 0.187485, valid_loss: 0.175683 ===\n",
      "=== epoch[24/100], loss: 0.187302, valid_loss: 0.175571 ===\n",
      "=== epoch[25/100], loss: 0.187114, valid_loss: 0.175604 ===\n",
      "=== epoch[26/100], loss: 0.186979, valid_loss: 0.175757 ===\n",
      "=== epoch[27/100], loss: 0.186884, valid_loss: 0.175940 ===\n",
      "=== epoch[28/100], loss: 0.186754, valid_loss: 0.175391 ===\n",
      "=== epoch[29/100], loss: 0.186586, valid_loss: 0.175521 ===\n",
      "=== epoch[30/100], loss: 0.186720, valid_loss: 0.176065 ===\n",
      "=== epoch[31/100], loss: 0.186738, valid_loss: 0.174301 ===\n",
      "=== epoch[32/100], loss: 0.185985, valid_loss: 0.174180 ===\n",
      "=== epoch[33/100], loss: 0.185826, valid_loss: 0.174123 ===\n",
      "=== epoch[34/100], loss: 0.185740, valid_loss: 0.174135 ===\n",
      "=== epoch[35/100], loss: 0.185687, valid_loss: 0.174099 ===\n",
      "=== epoch[36/100], loss: 0.185649, valid_loss: 0.174061 ===\n",
      "=== epoch[37/100], loss: 0.185622, valid_loss: 0.174030 ===\n",
      "=== epoch[38/100], loss: 0.185595, valid_loss: 0.173996 ===\n",
      "=== epoch[39/100], loss: 0.185560, valid_loss: 0.173985 ===\n",
      "=== epoch[40/100], loss: 0.185538, valid_loss: 0.173942 ===\n",
      "=== epoch[41/100], loss: 0.185458, valid_loss: 0.173830 ===\n",
      "=== epoch[42/100], loss: 0.185446, valid_loss: 0.173816 ===\n",
      "=== epoch[43/100], loss: 0.185428, valid_loss: 0.173815 ===\n",
      "=== epoch[44/100], loss: 0.185418, valid_loss: 0.173811 ===\n",
      "=== epoch[45/100], loss: 0.185409, valid_loss: 0.173809 ===\n",
      "=== epoch[46/100], loss: 0.185401, valid_loss: 0.173804 ===\n",
      "=== epoch[47/100], loss: 0.185395, valid_loss: 0.173799 ===\n",
      "=== epoch[48/100], loss: 0.185389, valid_loss: 0.173794 ===\n",
      "=== epoch[49/100], loss: 0.185383, valid_loss: 0.173790 ===\n",
      "=== epoch[50/100], loss: 0.185377, valid_loss: 0.173786 ===\n",
      "=== epoch[51/100], loss: 0.185353, valid_loss: 0.173780 ===\n",
      "=== epoch[52/100], loss: 0.185348, valid_loss: 0.173777 ===\n",
      "=== epoch[53/100], loss: 0.185347, valid_loss: 0.173776 ===\n",
      "=== epoch[54/100], loss: 0.185346, valid_loss: 0.173775 ===\n",
      "=== epoch[55/100], loss: 0.185345, valid_loss: 0.173773 ===\n",
      "=== epoch[56/100], loss: 0.185344, valid_loss: 0.173772 ===\n",
      "=== epoch[57/100], loss: 0.185342, valid_loss: 0.173771 ===\n",
      "=== epoch[58/100], loss: 0.185341, valid_loss: 0.173769 ===\n",
      "=== epoch[59/100], loss: 0.185340, valid_loss: 0.173768 ===\n",
      "=== epoch[60/100], loss: 0.185339, valid_loss: 0.173766 ===\n",
      "=== epoch[61/100], loss: 0.185334, valid_loss: 0.173766 ===\n",
      "=== epoch[62/100], loss: 0.185333, valid_loss: 0.173766 ===\n",
      "=== epoch[63/100], loss: 0.185333, valid_loss: 0.173765 ===\n",
      "=== epoch[64/100], loss: 0.185333, valid_loss: 0.173765 ===\n",
      "=== epoch[65/100], loss: 0.185333, valid_loss: 0.173765 ===\n",
      "=== epoch[66/100], loss: 0.185332, valid_loss: 0.173765 ===\n",
      "=== epoch[67/100], loss: 0.185332, valid_loss: 0.173764 ===\n",
      "=== epoch[68/100], loss: 0.185332, valid_loss: 0.173764 ===\n",
      "=== epoch[69/100], loss: 0.185332, valid_loss: 0.173764 ===\n",
      "=== epoch[70/100], loss: 0.185331, valid_loss: 0.173763 ===\n",
      "=== epoch[71/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[72/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[73/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[74/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[75/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[76/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[77/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[78/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[79/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[80/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[81/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[82/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[83/100], loss: 0.185330, valid_loss: 0.173763 ===\n",
      "=== epoch[84/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[85/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[86/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[87/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[88/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[89/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[90/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[91/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[92/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[93/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[94/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[95/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[96/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[97/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[98/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[99/100], loss: 0.185329, valid_loss: 0.173763 ===\n",
      "=== epoch[100/100], loss: 0.185329, valid_loss: 0.173763 ===\n"
     ]
    }
   ],
   "source": [
    "# Build linear model\n",
    "feature_columns = ['travel_time', 'uplink_mean_tt', 'downlink_mean_tt']\n",
    "\n",
    "B     = 256\n",
    "D_in  = g_region_temporal * len(feature_columns)\n",
    "D_hidden = D_in\n",
    "D_out = 1\n",
    "TRAIN_SET_RATIO = 0.80\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, D_hidden)\n",
    "        self.fc2 = nn.Linear(D_hidden, D_hidden)\n",
    "        self.fc3 = nn.Linear(D_hidden, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.dropout(self.fc1(x), 0.0))\n",
    "        x = F.relu(F.dropout(self.fc2(x), 0.2))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = FCNN()\n",
    "model.cuda()\n",
    "\n",
    "def rmse(y_hat, y):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return torch.sqrt(torch.mean((y - y_hat).pow(2)))\n",
    "\n",
    "def mape(y_hat, y):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return torch.mean(((y - y_hat) / y).abs())\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "loss_fn = mape\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 我们需要按link_ID切分dataset，因为不同link的数据不能看作一个时间序列\n",
    "datasets_train = []\n",
    "datasets_valid = []\n",
    "link_no = ds_train_full.link_ID.unique().shape[0]\n",
    "counter = 0\n",
    "for link_ID, link_ds in ds_train_full.groupby('link_ID'):\n",
    "    counter += 1\n",
    "    if counter < link_no * TRAIN_SET_RATIO:\n",
    "        datasets_train.append(PandasDataset(link_ds, feature_columns))\n",
    "    else:\n",
    "        datasets_valid.append(PandasDataset(link_ds, feature_columns))\n",
    "\n",
    "print('train set size:', len(datasets_train) * len(datasets_train[0]))\n",
    "print('valid set size:', len(datasets_valid) * len(datasets_valid[0]))\n",
    "dataset_train = data.ConcatDataset(datasets_train)\n",
    "dataset_valid = data.ConcatDataset(datasets_valid)\n",
    "loader_train = data.DataLoader(dataset_train, batch_size=B, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "loader_valid = data.DataLoader(dataset_valid, batch_size=B, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "\n",
    "def validate():\n",
    "    eval_running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batch in enumerate(loader_valid):\n",
    "        x_batch = Variable(sample_batch['feature']).cuda()\n",
    "        y_batch = Variable(sample_batch['label']).cuda()\n",
    "        y_batch_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        eval_running_loss += loss.data[0]\n",
    "        counter += 1\n",
    "        \n",
    "    return eval_running_loss / counter\n",
    "\n",
    "num_epochs = 100\n",
    "epoch_loss_records = []\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "for epoch in range(num_epochs):\n",
    "    lr_scheduler.step()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batch in enumerate(loader_train):\n",
    "        x_batch = Variable(sample_batch['feature']).cuda()\n",
    "        y_batch = Variable(sample_batch['label']).cuda()\n",
    "                \n",
    "        # forward\n",
    "        y_batch_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        counter += 1\n",
    "\n",
    "    epoch_mean_loss = running_loss / counter\n",
    "    eval_mean_loss = validate()\n",
    "    print('=== epoch[{}/{}], loss: {:.6f}, valid_loss: {:.6f} ==='\n",
    "                  .format(epoch + 1, num_epochs, epoch_mean_loss, eval_mean_loss))\n",
    "    epoch_loss_records.append(epoch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
