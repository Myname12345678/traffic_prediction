{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model as baseline (use temporal features only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:27:08.570863Z",
     "start_time": "2018-02-27T07:27:08.129621Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import lab.setup\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "g_region_temporal = 5\n",
    "g_region_spatial  = 1\n",
    "g_start_date = '2016-03-03'\n",
    "g_end_date   = '2016-03-03'\n",
    "g_start_time = '{} 00:00:00'.format(g_start_date)\n",
    "g_end_time   = '{} 23:59:59'.format(g_end_date)\n",
    "\n",
    "DATA_PATH = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## extract traffic time in (g_region_temporal, g_region_spatial) as feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T07:56:33.951039Z",
     "start_time": "2018-02-21T07:56:33.945588Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T09:32:14.245770Z",
     "start_time": "2018-02-26T09:32:08.527910Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topology = pd.read_csv(f'{DATA_PATH}/gy_contest_link_top.txt', sep=';')\n",
    "ds_train_raw = pd.read_csv(\n",
    "    f'{DATA_PATH}/gy_contest_link_traveltime_training_data.txt', \n",
    "    sep=';', \n",
    "    parse_dates=['date'], \n",
    "    index_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T09:32:14.743231Z",
     "start_time": "2018-02-26T09:32:14.246947Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sampling:  (7662384, 3)\n",
      "after sampling:  (86124, 3)\n"
     ]
    }
   ],
   "source": [
    "print('before sampling: ', ds_train_raw.shape)\n",
    "ds_train_sample = ds_train_raw.loc[g_start_date:g_end_date]\n",
    "print('after sampling: ', ds_train_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T09:55:43.355167Z",
     "start_time": "2018-02-23T09:55:43.353611Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train_sample\n",
    "# ds_train_sample.groupby('link_ID').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T04:51:50.224446Z",
     "start_time": "2018-02-14T04:51:50.218311Z"
    },
    "hidden": true
   },
   "source": [
    "### build spatial&& temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T10:52:15.513942Z",
     "start_time": "2018-02-23T10:52:12.734443Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# join topology\n",
    "ds_train_topo = ds_train.reset_index().merge(topology, on=['link_ID'], how='left')\n",
    "\n",
    "# parse time interval&& sort dataset\n",
    "def _parse_time_interval(string):\n",
    "    return pd.to_datetime(string[1:20])\n",
    "\n",
    "ds_train_topo['time_intv'] = ds_train_topo.time_interval.apply(_parse_time_interval)\n",
    "ds_train_topo = ds_train_topo.sort_values(by=['link_ID', 'time_intv']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T11:16:06.323746Z",
     "start_time": "2018-02-23T11:16:05.969682Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fillna\n",
    "# 先生成一个完整的link_ID+time_intv的grid，然后把数据集join上来，最后fillna\n",
    "s_link_ID = ds_train_topo.link_ID.unique()\n",
    "s_time_intv = pd.date_range(\n",
    "    pd.to_datetime(g_start_time), pd.to_datetime(g_end_time), freq='2min').to_series().reset_index(drop=True)\n",
    "\n",
    "ds_left  = pd.DataFrame()\n",
    "ds_right = pd.DataFrame()\n",
    "ds_left['link_ID'] = s_link_ID\n",
    "ds_left['cartesian_key'] = 0\n",
    "ds_right['time_intv'] = s_time_intv\n",
    "ds_right['cartesian_key'] = 0\n",
    "\n",
    "ds_full = pd.merge(ds_left, ds_right, on='cartesian_key').drop('cartesian_key', axis=1)\n",
    "ds_full = pd.merge(ds_full, ds_train_topo, on=['link_ID', 'time_intv'], how='left')\n",
    "# 1, date, time_interval字段无用，不需要补齐\n",
    "# 2, in_links, out_links对每个link_ID必然相同，无脑补齐\n",
    "# 3, travel_time优先使用小的时间戳去补齐大的时间戳，即ffill，对开头的缺失，再使用bfill\n",
    "fill_col_mask = ['link_ID', 'travel_time', 'in_links', 'out_links']\n",
    "ds_full[fill_col_mask] = ds_full[fill_col_mask].groupby(by='link_ID').ffill().bfill()\n",
    "fill_col_mask = ['date', 'time_interval']\n",
    "ds_full[fill_col_mask] = ds_full[fill_col_mask].fillna('N/A')\n",
    "ds_full['filled'] = ds_full.date == 'N/A' # 表示是补齐出来的数据\n",
    "ds_full.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T11:16:07.275992Z",
     "start_time": "2018-02-23T11:16:06.527818Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# join 1-order uplinks&& downlinks\n",
    "def _append_col_prefix(df, prefix):\n",
    "    df.columns = [prefix + '_' + str(i) for i in df.columns]\n",
    "    \n",
    "df_uplinks_arr   = ds_full.in_links.str.split('#', expand=True).fillna(0).astype('uint64')\n",
    "df_downlinks_arr = ds_full.out_links.str.split('#', expand=True).fillna(0).astype('uint64')\n",
    "_append_col_prefix(df_uplinks_arr, 'uplink')\n",
    "_append_col_prefix(df_downlinks_arr, 'downlink')\n",
    "\n",
    "ds_full = pd.concat([ds_full, df_uplinks_arr], axis=1)\n",
    "ds_full = pd.concat([ds_full, df_downlinks_arr], axis=1)\n",
    "\n",
    "# join travel time of uplinks&& downlinks(may be null in absence of nbr link)\n",
    "for nbr_type in ['uplink', 'downlink']:\n",
    "    for nbr_idx in range(0, 4): # @TODO: magic\n",
    "        ds_full = pd.merge(\n",
    "            ds_full, ds_full[['link_ID', 'time_intv', 'travel_time']], \n",
    "            left_on=['{}_{}'.format(nbr_type, nbr_idx), 'time_intv'], \n",
    "            right_on=['link_ID', 'time_intv'], how='left', \n",
    "            suffixes=['', '_{}_{}'.format(nbr_type, nbr_idx)])\n",
    "\n",
    "# get average travel time of nbrs for each order\n",
    "nbr_tt_columns = []\n",
    "for nbr_idx in range(0, 4):\n",
    "    nbr_tt_columns.append('travel_time_{}_{}'.format('{}', nbr_idx))\n",
    "    \n",
    "for nbr_type in ['uplink', 'downlink']:\n",
    "    ds_full['{}_mean_tt'.format(nbr_type)] \\\n",
    "        = ds_full[[col.format(nbr_type) for col in nbr_tt_columns]].mean(axis=1)\n",
    "\n",
    "# drop temp columns\n",
    "temp_cols = []\n",
    "for nbr_type in ['uplink', 'downlink']:\n",
    "    for nbr_idx in range(0, 4):\n",
    "        temp_cols.append('link_ID_{}_{}'.format(nbr_type, nbr_idx))\n",
    "        temp_cols.append('travel_time_{}_{}'.format(nbr_type, nbr_idx))\n",
    "\n",
    "ds_full = ds_full.drop(temp_cols, axis=1)\n",
    "ds_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T11:16:56.948041Z",
     "start_time": "2018-02-23T11:16:56.142976Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_full.to_csv('dataset/ds_filled_s1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:27:14.146964Z",
     "start_time": "2018-02-27T07:27:13.933917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_ID</th>\n",
       "      <th>time_intv</th>\n",
       "      <th>date</th>\n",
       "      <th>time_interval</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>in_links</th>\n",
       "      <th>out_links</th>\n",
       "      <th>filled</th>\n",
       "      <th>uplink_0</th>\n",
       "      <th>uplink_1</th>\n",
       "      <th>uplink_2</th>\n",
       "      <th>uplink_3</th>\n",
       "      <th>downlink_0</th>\n",
       "      <th>downlink_1</th>\n",
       "      <th>downlink_2</th>\n",
       "      <th>downlink_3</th>\n",
       "      <th>uplink_mean_tt</th>\n",
       "      <th>downlink_mean_tt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2016-03-03 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4377906282541600514</td>\n",
       "      <td>4377906280763800514</td>\n",
       "      <td>True</td>\n",
       "      <td>4377906282541600514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4377906280763800514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.4</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               link_ID            time_intv date time_interval  travel_time  \\\n",
       "0  3377906280028510514  2016-03-03 00:00:00  NaN           NaN          5.1   \n",
       "\n",
       "              in_links            out_links  filled             uplink_0  \\\n",
       "0  4377906282541600514  4377906280763800514    True  4377906282541600514   \n",
       "\n",
       "   uplink_1  uplink_2  uplink_3           downlink_0  downlink_1  downlink_2  \\\n",
       "0         0         0         0  4377906280763800514           0           0   \n",
       "\n",
       "   downlink_3  uplink_mean_tt  downlink_mean_tt  \n",
       "0           0            55.4               8.4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load prepared data\n",
    "# ds_train_full = pd.read_csv('dataset/ds_s1t5_flatten.csv', dtype={'link_ID':'uint64'}, low_memory=False)\n",
    "ds_train_full = pd.read_csv('dataset/ds_filled_s1.csv', dtype={'link_ID':'uint64'}, low_memory=False)\n",
    "ds_train_full.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:27:47.714207Z",
     "start_time": "2018-02-27T07:27:47.693641Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PandasDataset(data.Dataset):\n",
    "    def __init__(self, df, feature_columns):\n",
    "        self.df = df[feature_columns].astype('float32')\n",
    "        self.dataset = self.df.values\n",
    "        self.temporal_order = g_region_temporal\n",
    "        self.feature_size = self.temporal_order * len(feature_columns)\n",
    "        self.len = self.df.shape[0] - (self.temporal_order - 1) - 1\n",
    "        self.label_index = self.df.columns.tolist().index('travel_time')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_vec = self.dataset[idx:idx+self.temporal_order].reshape(self.feature_size)\n",
    "        label_vec   = self.dataset[idx+self.temporal_order][self.label_index:self.label_index+1]\n",
    "        \n",
    "        return {'feature': feature_vec, 'label': label_vec}\n",
    "\n",
    "def collate(batch):\n",
    "    \"Puts each data field into a tensor with outer dimension batch size\"\n",
    "    feature_batch = torch.stack([torch.from_numpy(f['feature']) for f in batch], 0)\n",
    "    label_batch = torch.stack([torch.from_numpy(f['label']) for f in batch], 0)\n",
    "    return {\n",
    "        'feature': feature_batch, \n",
    "        'label': label_batch\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:32:48.058480Z",
     "start_time": "2018-02-27T07:31:52.235663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 75075\n",
      "valid set size: 19305\n",
      "=== epoch[1/100], loss: 0.263065, valid_loss: 0.197000 ===\n",
      "=== epoch[2/100], loss: 0.199741, valid_loss: 0.181806 ===\n",
      "=== epoch[3/100], loss: 0.193451, valid_loss: 0.180198 ===\n",
      "=== epoch[4/100], loss: 0.193472, valid_loss: 0.179915 ===\n",
      "=== epoch[5/100], loss: 0.193625, valid_loss: 0.180520 ===\n",
      "=== epoch[6/100], loss: 0.193703, valid_loss: 0.180478 ===\n",
      "=== epoch[7/100], loss: 0.193730, valid_loss: 0.180535 ===\n",
      "=== epoch[8/100], loss: 0.193810, valid_loss: 0.180543 ===\n",
      "=== epoch[9/100], loss: 0.193827, valid_loss: 0.180465 ===\n",
      "=== epoch[10/100], loss: 0.193810, valid_loss: 0.180499 ===\n",
      "=== epoch[11/100], loss: 0.191241, valid_loss: 0.181137 ===\n",
      "=== epoch[12/100], loss: 0.191365, valid_loss: 0.181349 ===\n",
      "=== epoch[13/100], loss: 0.191389, valid_loss: 0.181485 ===\n",
      "=== epoch[14/100], loss: 0.191407, valid_loss: 0.181682 ===\n",
      "=== epoch[15/100], loss: 0.191424, valid_loss: 0.181723 ===\n",
      "=== epoch[16/100], loss: 0.191414, valid_loss: 0.181793 ===\n",
      "=== epoch[17/100], loss: 0.191395, valid_loss: 0.181684 ===\n",
      "=== epoch[18/100], loss: 0.191388, valid_loss: 0.181745 ===\n",
      "=== epoch[19/100], loss: 0.191395, valid_loss: 0.181765 ===\n",
      "=== epoch[20/100], loss: 0.191392, valid_loss: 0.181716 ===\n",
      "=== epoch[21/100], loss: 0.190770, valid_loss: 0.178992 ===\n",
      "=== epoch[22/100], loss: 0.190607, valid_loss: 0.178984 ===\n",
      "=== epoch[23/100], loss: 0.190592, valid_loss: 0.178973 ===\n",
      "=== epoch[24/100], loss: 0.190588, valid_loss: 0.178976 ===\n",
      "=== epoch[25/100], loss: 0.190587, valid_loss: 0.178989 ===\n",
      "=== epoch[26/100], loss: 0.190587, valid_loss: 0.178988 ===\n",
      "=== epoch[27/100], loss: 0.190585, valid_loss: 0.178976 ===\n",
      "=== epoch[28/100], loss: 0.190583, valid_loss: 0.178969 ===\n",
      "=== epoch[29/100], loss: 0.190583, valid_loss: 0.178975 ===\n",
      "=== epoch[30/100], loss: 0.190584, valid_loss: 0.178977 ===\n",
      "=== epoch[31/100], loss: 0.190393, valid_loss: 0.179060 ===\n",
      "=== epoch[32/100], loss: 0.190394, valid_loss: 0.179063 ===\n",
      "=== epoch[33/100], loss: 0.190393, valid_loss: 0.179068 ===\n",
      "=== epoch[34/100], loss: 0.190392, valid_loss: 0.179067 ===\n",
      "=== epoch[35/100], loss: 0.190391, valid_loss: 0.179068 ===\n",
      "=== epoch[36/100], loss: 0.190391, valid_loss: 0.179063 ===\n",
      "=== epoch[37/100], loss: 0.190390, valid_loss: 0.179063 ===\n",
      "=== epoch[38/100], loss: 0.190391, valid_loss: 0.179064 ===\n",
      "=== epoch[39/100], loss: 0.190390, valid_loss: 0.178979 ===\n",
      "=== epoch[40/100], loss: 0.190389, valid_loss: 0.179058 ===\n",
      "=== epoch[41/100], loss: 0.190343, valid_loss: 0.178985 ===\n",
      "=== epoch[42/100], loss: 0.190339, valid_loss: 0.178970 ===\n",
      "=== epoch[43/100], loss: 0.190339, valid_loss: 0.178963 ===\n",
      "=== epoch[44/100], loss: 0.190338, valid_loss: 0.178960 ===\n",
      "=== epoch[45/100], loss: 0.190339, valid_loss: 0.178961 ===\n",
      "=== epoch[46/100], loss: 0.190338, valid_loss: 0.178959 ===\n",
      "=== epoch[47/100], loss: 0.190338, valid_loss: 0.178958 ===\n",
      "=== epoch[48/100], loss: 0.190338, valid_loss: 0.178958 ===\n",
      "=== epoch[49/100], loss: 0.190338, valid_loss: 0.178957 ===\n",
      "=== epoch[50/100], loss: 0.190338, valid_loss: 0.178956 ===\n",
      "=== epoch[51/100], loss: 0.190328, valid_loss: 0.178954 ===\n",
      "=== epoch[52/100], loss: 0.190328, valid_loss: 0.178953 ===\n",
      "=== epoch[53/100], loss: 0.190328, valid_loss: 0.178951 ===\n",
      "=== epoch[54/100], loss: 0.190328, valid_loss: 0.178950 ===\n",
      "=== epoch[55/100], loss: 0.190328, valid_loss: 0.178949 ===\n",
      "=== epoch[56/100], loss: 0.190328, valid_loss: 0.178948 ===\n",
      "=== epoch[57/100], loss: 0.190328, valid_loss: 0.178948 ===\n",
      "=== epoch[58/100], loss: 0.190328, valid_loss: 0.178947 ===\n",
      "=== epoch[59/100], loss: 0.190327, valid_loss: 0.178946 ===\n",
      "=== epoch[60/100], loss: 0.190327, valid_loss: 0.178946 ===\n",
      "=== epoch[61/100], loss: 0.190325, valid_loss: 0.178946 ===\n",
      "=== epoch[62/100], loss: 0.190325, valid_loss: 0.178946 ===\n",
      "=== epoch[63/100], loss: 0.190325, valid_loss: 0.178946 ===\n",
      "=== epoch[64/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[65/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[66/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[67/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[68/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[69/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[70/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[71/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[72/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[73/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[74/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[75/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[76/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[77/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[78/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[79/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[80/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[81/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[82/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[83/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[84/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[85/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[86/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[87/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[88/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[89/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[90/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[91/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[92/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[93/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[94/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[95/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[96/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[97/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[98/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[99/100], loss: 0.190325, valid_loss: 0.178945 ===\n",
      "=== epoch[100/100], loss: 0.190325, valid_loss: 0.178945 ===\n"
     ]
    }
   ],
   "source": [
    "# Build linear model\n",
    "feature_columns = ['travel_time']\n",
    "\n",
    "B     = 256\n",
    "D_in  = g_region_temporal * len(feature_columns)\n",
    "D_out = 1\n",
    "TRAIN_SET_RATIO = 0.8\n",
    "\n",
    "# Single linear layer\n",
    "model = torch.nn.Linear(D_in, D_out)\n",
    "model.cuda()\n",
    "\n",
    "def rmse(y_hat, y):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return torch.sqrt(torch.mean((y - y_hat).pow(2)))\n",
    "\n",
    "def mape(y_hat, y):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return torch.mean(((y - y_hat) / y).abs())\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "loss_fn = mape\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 我们需要按link_ID切分dataset，因为不同link的数据不能看作一个时间序列\n",
    "datasets_train = []\n",
    "datasets_valid = []\n",
    "link_no = ds_train_full.link_ID.unique().shape[0]\n",
    "counter = 0\n",
    "for link_ID, link_ds in ds_train_full.groupby('link_ID'):\n",
    "    counter += 1\n",
    "    if counter < link_no * TRAIN_SET_RATIO:\n",
    "        datasets_train.append(PandasDataset(link_ds, feature_columns))\n",
    "    else:\n",
    "        datasets_valid.append(PandasDataset(link_ds, feature_columns))\n",
    "\n",
    "print('train set size:', len(datasets_train) * len(datasets_train[0]))\n",
    "print('valid set size:', len(datasets_valid) * len(datasets_valid[0]))\n",
    "dataset_train = data.ConcatDataset(datasets_train)\n",
    "dataset_valid = data.ConcatDataset(datasets_valid)\n",
    "loader_train = data.DataLoader(dataset_train, batch_size=B, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "loader_valid = data.DataLoader(dataset_valid, batch_size=B, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "\n",
    "def validate():\n",
    "    eval_running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batch in enumerate(loader_valid):\n",
    "        x_batch = Variable(sample_batch['feature']).cuda()\n",
    "        y_batch = Variable(sample_batch['label']).cuda()\n",
    "        y_batch_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        eval_running_loss += loss.data[0]\n",
    "        counter += 1\n",
    "        \n",
    "    return eval_running_loss / counter\n",
    "\n",
    "num_epochs = 100\n",
    "epoch_loss_records = []\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "for epoch in range(num_epochs):\n",
    "    lr_scheduler.step()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batch in enumerate(loader_train):\n",
    "        x_batch = Variable(sample_batch['feature']).cuda()\n",
    "        y_batch = Variable(sample_batch['label']).cuda()\n",
    "\n",
    "        # forward\n",
    "        y_batch_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        counter += 1\n",
    "\n",
    "    epoch_mean_loss = running_loss / counter\n",
    "    eval_mean_loss = validate()\n",
    "    print('=== epoch[{}/{}], loss: {:.6f}, valid_loss: {:.6f} ==='\n",
    "                  .format(epoch + 1, num_epochs, epoch_mean_loss, eval_mean_loss))\n",
    "    epoch_loss_records.append(epoch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
