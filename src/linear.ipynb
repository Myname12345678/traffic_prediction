{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:28:30.466254Z",
     "start_time": "2018-02-27T07:28:29.922369Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import lab.setup\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "g_region_temporal = 5\n",
    "g_region_spatial  = 1\n",
    "g_start_date = '2016-03-03'\n",
    "g_end_date   = '2016-03-03'\n",
    "g_start_time = '{} 00:00:00'.format(g_start_date)\n",
    "g_end_time   = '{} 23:59:59'.format(g_end_date)\n",
    "\n",
    "DATA_PATH = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## extract traffic time in (g_region_temporal, g_region_spatial) as feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T07:56:33.951039Z",
     "start_time": "2018-02-21T07:56:33.945588Z"
    },
    "hidden": true
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T09:32:14.245770Z",
     "start_time": "2018-02-26T09:32:08.527910Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topology = pd.read_csv(f'{DATA_PATH}/gy_contest_link_top.txt', sep=';')\n",
    "ds_train_raw = pd.read_csv(\n",
    "    f'{DATA_PATH}/gy_contest_link_traveltime_training_data.txt', \n",
    "    sep=';', \n",
    "    parse_dates=['date'], \n",
    "    index_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T09:32:14.743231Z",
     "start_time": "2018-02-26T09:32:14.246947Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sampling:  (7662384, 3)\n",
      "after sampling:  (86124, 3)\n"
     ]
    }
   ],
   "source": [
    "print('before sampling: ', ds_train_raw.shape)\n",
    "ds_train_sample = ds_train_raw.loc[g_start_date:g_end_date]\n",
    "print('after sampling: ', ds_train_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T09:55:43.355167Z",
     "start_time": "2018-02-23T09:55:43.353611Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train_sample\n",
    "# ds_train_sample.groupby('link_ID').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T04:51:50.224446Z",
     "start_time": "2018-02-14T04:51:50.218311Z"
    },
    "hidden": true
   },
   "source": [
    "### build spatial&& temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T10:52:15.513942Z",
     "start_time": "2018-02-23T10:52:12.734443Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# join topology\n",
    "ds_train_topo = ds_train.reset_index().merge(topology, on=['link_ID'], how='left')\n",
    "\n",
    "# parse time interval&& sort dataset\n",
    "def _parse_time_interval(string):\n",
    "    return pd.to_datetime(string[1:20])\n",
    "\n",
    "ds_train_topo['time_intv'] = ds_train_topo.time_interval.apply(_parse_time_interval)\n",
    "ds_train_topo = ds_train_topo.sort_values(by=['link_ID', 'time_intv']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T11:16:06.323746Z",
     "start_time": "2018-02-23T11:16:05.969682Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fillna\n",
    "# 先生成一个完整的link_ID+time_intv的grid，然后把数据集join上来，最后fillna\n",
    "s_link_ID = ds_train_topo.link_ID.unique()\n",
    "s_time_intv = pd.date_range(\n",
    "    pd.to_datetime(g_start_time), pd.to_datetime(g_end_time), freq='2min').to_series().reset_index(drop=True)\n",
    "\n",
    "ds_left  = pd.DataFrame()\n",
    "ds_right = pd.DataFrame()\n",
    "ds_left['link_ID'] = s_link_ID\n",
    "ds_left['cartesian_key'] = 0\n",
    "ds_right['time_intv'] = s_time_intv\n",
    "ds_right['cartesian_key'] = 0\n",
    "\n",
    "ds_full = pd.merge(ds_left, ds_right, on='cartesian_key').drop('cartesian_key', axis=1)\n",
    "ds_full = pd.merge(ds_full, ds_train_topo, on=['link_ID', 'time_intv'], how='left')\n",
    "# 1, date, time_interval字段无用，不需要补齐\n",
    "# 2, in_links, out_links对每个link_ID必然相同，无脑补齐\n",
    "# 3, travel_time优先使用小的时间戳去补齐大的时间戳，即ffill，对开头的缺失，再使用bfill\n",
    "fill_col_mask = ['link_ID', 'travel_time', 'in_links', 'out_links']\n",
    "ds_full[fill_col_mask] = ds_full[fill_col_mask].groupby(by='link_ID').ffill().bfill()\n",
    "fill_col_mask = ['date', 'time_interval']\n",
    "ds_full[fill_col_mask] = ds_full[fill_col_mask].fillna('N/A')\n",
    "ds_full['filled'] = ds_full.date == 'N/A' # 表示是补齐出来的数据\n",
    "ds_full.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T11:16:07.275992Z",
     "start_time": "2018-02-23T11:16:06.527818Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# join 1-order uplinks&& downlinks\n",
    "def _append_col_prefix(df, prefix):\n",
    "    df.columns = [prefix + '_' + str(i) for i in df.columns]\n",
    "    \n",
    "df_uplinks_arr   = ds_full.in_links.str.split('#', expand=True).fillna(0).astype('uint64')\n",
    "df_downlinks_arr = ds_full.out_links.str.split('#', expand=True).fillna(0).astype('uint64')\n",
    "_append_col_prefix(df_uplinks_arr, 'uplink')\n",
    "_append_col_prefix(df_downlinks_arr, 'downlink')\n",
    "\n",
    "ds_full = pd.concat([ds_full, df_uplinks_arr], axis=1)\n",
    "ds_full = pd.concat([ds_full, df_downlinks_arr], axis=1)\n",
    "\n",
    "# join travel time of uplinks&& downlinks(may be null in absence of nbr link)\n",
    "for nbr_type in ['uplink', 'downlink']:\n",
    "    for nbr_idx in range(0, 4): # @TODO: magic\n",
    "        ds_full = pd.merge(\n",
    "            ds_full, ds_full[['link_ID', 'time_intv', 'travel_time']], \n",
    "            left_on=['{}_{}'.format(nbr_type, nbr_idx), 'time_intv'], \n",
    "            right_on=['link_ID', 'time_intv'], how='left', \n",
    "            suffixes=['', '_{}_{}'.format(nbr_type, nbr_idx)])\n",
    "\n",
    "# get average travel time of nbrs for each order\n",
    "nbr_tt_columns = []\n",
    "for nbr_idx in range(0, 4):\n",
    "    nbr_tt_columns.append('travel_time_{}_{}'.format('{}', nbr_idx))\n",
    "    \n",
    "for nbr_type in ['uplink', 'downlink']:\n",
    "    ds_full['{}_mean_tt'.format(nbr_type)] \\\n",
    "        = ds_full[[col.format(nbr_type) for col in nbr_tt_columns]].mean(axis=1)\n",
    "\n",
    "# drop temp columns\n",
    "temp_cols = []\n",
    "for nbr_type in ['uplink', 'downlink']:\n",
    "    for nbr_idx in range(0, 4):\n",
    "        temp_cols.append('link_ID_{}_{}'.format(nbr_type, nbr_idx))\n",
    "        temp_cols.append('travel_time_{}_{}'.format(nbr_type, nbr_idx))\n",
    "\n",
    "ds_full = ds_full.drop(temp_cols, axis=1)\n",
    "ds_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-23T11:16:56.948041Z",
     "start_time": "2018-02-23T11:16:56.142976Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_full.to_csv('dataset/ds_filled_s1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:28:33.273309Z",
     "start_time": "2018-02-27T07:28:33.042508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_ID</th>\n",
       "      <th>time_intv</th>\n",
       "      <th>date</th>\n",
       "      <th>time_interval</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>in_links</th>\n",
       "      <th>out_links</th>\n",
       "      <th>filled</th>\n",
       "      <th>uplink_0</th>\n",
       "      <th>uplink_1</th>\n",
       "      <th>uplink_2</th>\n",
       "      <th>uplink_3</th>\n",
       "      <th>downlink_0</th>\n",
       "      <th>downlink_1</th>\n",
       "      <th>downlink_2</th>\n",
       "      <th>downlink_3</th>\n",
       "      <th>uplink_mean_tt</th>\n",
       "      <th>downlink_mean_tt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3377906280028510514</td>\n",
       "      <td>2016-03-03 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4377906282541600514</td>\n",
       "      <td>4377906280763800514</td>\n",
       "      <td>True</td>\n",
       "      <td>4377906282541600514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4377906280763800514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.4</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               link_ID            time_intv date time_interval  travel_time  \\\n",
       "0  3377906280028510514  2016-03-03 00:00:00  NaN           NaN          5.1   \n",
       "\n",
       "              in_links            out_links  filled             uplink_0  \\\n",
       "0  4377906282541600514  4377906280763800514    True  4377906282541600514   \n",
       "\n",
       "   uplink_1  uplink_2  uplink_3           downlink_0  downlink_1  downlink_2  \\\n",
       "0         0         0         0  4377906280763800514           0           0   \n",
       "\n",
       "   downlink_3  uplink_mean_tt  downlink_mean_tt  \n",
       "0           0            55.4               8.4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load prepared data\n",
    "# ds_train_full = pd.read_csv('dataset/ds_s1t5_flatten.csv', dtype={'link_ID':'uint64'}, low_memory=False)\n",
    "ds_train_full = pd.read_csv('dataset/ds_filled_s1.csv', dtype={'link_ID':'uint64'}, low_memory=False)\n",
    "ds_train_full.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:28:33.448732Z",
     "start_time": "2018-02-27T07:28:33.424117Z"
    }
   },
   "outputs": [],
   "source": [
    "class PandasDataset(data.Dataset):\n",
    "    def __init__(self, df, feature_columns):\n",
    "        self.df = df[feature_columns].astype('float32')\n",
    "        self.dataset = self.df.values\n",
    "        self.temporal_order = g_region_temporal\n",
    "        self.feature_size = self.temporal_order * len(feature_columns)\n",
    "        self.len = self.df.shape[0] - (self.temporal_order - 1) - 1\n",
    "        self.label_index = self.df.columns.tolist().index('travel_time')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_vec = self.dataset[idx:idx+self.temporal_order].reshape(self.feature_size)\n",
    "        label_vec   = self.dataset[idx+self.temporal_order][self.label_index:self.label_index+1]\n",
    "        \n",
    "        return {'feature': feature_vec, 'label': label_vec}\n",
    "\n",
    "def collate(batch):\n",
    "    \"Puts each data field into a tensor with outer dimension batch size\"\n",
    "    feature_batch = torch.stack([torch.from_numpy(f['feature']) for f in batch], 0)\n",
    "    label_batch = torch.stack([torch.from_numpy(f['label']) for f in batch], 0)\n",
    "    return {\n",
    "        'feature': feature_batch, \n",
    "        'label': label_batch\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T07:31:56.181146Z",
     "start_time": "2018-02-27T07:31:09.750692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 75075\n",
      "valid set size: 19305\n",
      "=== epoch[1/100], loss: 0.517381, valid_loss: 0.270519 ===\n",
      "=== epoch[2/100], loss: 0.383149, valid_loss: 0.262476 ===\n",
      "=== epoch[3/100], loss: 0.305735, valid_loss: 0.234680 ===\n",
      "=== epoch[4/100], loss: 0.322688, valid_loss: 0.411402 ===\n",
      "=== epoch[5/100], loss: 0.395721, valid_loss: 0.390955 ===\n",
      "=== epoch[6/100], loss: 0.369979, valid_loss: 0.440615 ===\n",
      "=== epoch[7/100], loss: 0.381223, valid_loss: 0.437316 ===\n",
      "=== epoch[8/100], loss: 0.369993, valid_loss: 0.435582 ===\n",
      "=== epoch[9/100], loss: 0.368392, valid_loss: 0.426705 ===\n",
      "=== epoch[10/100], loss: 0.368573, valid_loss: 0.426254 ===\n",
      "=== epoch[11/100], loss: 0.256012, valid_loss: 0.184671 ===\n",
      "=== epoch[12/100], loss: 0.203575, valid_loss: 0.184315 ===\n",
      "=== epoch[13/100], loss: 0.202377, valid_loss: 0.182915 ===\n",
      "=== epoch[14/100], loss: 0.203439, valid_loss: 0.182633 ===\n",
      "=== epoch[15/100], loss: 0.206585, valid_loss: 0.188532 ===\n",
      "=== epoch[16/100], loss: 0.206959, valid_loss: 0.182748 ===\n",
      "=== epoch[17/100], loss: 0.208196, valid_loss: 0.198134 ===\n",
      "=== epoch[18/100], loss: 0.208475, valid_loss: 0.183312 ===\n",
      "=== epoch[19/100], loss: 0.207952, valid_loss: 0.192967 ===\n",
      "=== epoch[20/100], loss: 0.205224, valid_loss: 0.181676 ===\n",
      "=== epoch[21/100], loss: 0.192733, valid_loss: 0.182523 ===\n",
      "=== epoch[22/100], loss: 0.192535, valid_loss: 0.180768 ===\n",
      "=== epoch[23/100], loss: 0.192258, valid_loss: 0.180241 ===\n",
      "=== epoch[24/100], loss: 0.192195, valid_loss: 0.180347 ===\n",
      "=== epoch[25/100], loss: 0.192611, valid_loss: 0.181440 ===\n",
      "=== epoch[26/100], loss: 0.192702, valid_loss: 0.181286 ===\n",
      "=== epoch[27/100], loss: 0.192082, valid_loss: 0.180732 ===\n",
      "=== epoch[28/100], loss: 0.191884, valid_loss: 0.180391 ===\n",
      "=== epoch[29/100], loss: 0.192776, valid_loss: 0.181428 ===\n",
      "=== epoch[30/100], loss: 0.192569, valid_loss: 0.181307 ===\n",
      "=== epoch[31/100], loss: 0.193308, valid_loss: 0.179436 ===\n",
      "=== epoch[32/100], loss: 0.191662, valid_loss: 0.179380 ===\n",
      "=== epoch[33/100], loss: 0.191605, valid_loss: 0.179425 ===\n",
      "=== epoch[34/100], loss: 0.191620, valid_loss: 0.179467 ===\n",
      "=== epoch[35/100], loss: 0.191796, valid_loss: 0.179525 ===\n",
      "=== epoch[36/100], loss: 0.191747, valid_loss: 0.179508 ===\n",
      "=== epoch[37/100], loss: 0.191773, valid_loss: 0.179572 ===\n",
      "=== epoch[38/100], loss: 0.191808, valid_loss: 0.179587 ===\n",
      "=== epoch[39/100], loss: 0.191764, valid_loss: 0.179601 ===\n",
      "=== epoch[40/100], loss: 0.191913, valid_loss: 0.179628 ===\n",
      "=== epoch[41/100], loss: 0.191017, valid_loss: 0.179371 ===\n",
      "=== epoch[42/100], loss: 0.190635, valid_loss: 0.178888 ===\n",
      "=== epoch[43/100], loss: 0.190653, valid_loss: 0.179373 ===\n",
      "=== epoch[44/100], loss: 0.190638, valid_loss: 0.178872 ===\n",
      "=== epoch[45/100], loss: 0.190714, valid_loss: 0.179387 ===\n",
      "=== epoch[46/100], loss: 0.190703, valid_loss: 0.178877 ===\n",
      "=== epoch[47/100], loss: 0.190730, valid_loss: 0.179381 ===\n",
      "=== epoch[48/100], loss: 0.190702, valid_loss: 0.178878 ===\n",
      "=== epoch[49/100], loss: 0.190733, valid_loss: 0.179372 ===\n",
      "=== epoch[50/100], loss: 0.190706, valid_loss: 0.178880 ===\n",
      "=== epoch[51/100], loss: 0.190504, valid_loss: 0.179040 ===\n",
      "=== epoch[52/100], loss: 0.190435, valid_loss: 0.179044 ===\n",
      "=== epoch[53/100], loss: 0.190437, valid_loss: 0.179053 ===\n",
      "=== epoch[54/100], loss: 0.190439, valid_loss: 0.179058 ===\n",
      "=== epoch[55/100], loss: 0.190443, valid_loss: 0.179061 ===\n",
      "=== epoch[56/100], loss: 0.190441, valid_loss: 0.179069 ===\n",
      "=== epoch[57/100], loss: 0.190445, valid_loss: 0.179069 ===\n",
      "=== epoch[58/100], loss: 0.190442, valid_loss: 0.179078 ===\n",
      "=== epoch[59/100], loss: 0.190447, valid_loss: 0.179073 ===\n",
      "=== epoch[60/100], loss: 0.190444, valid_loss: 0.179079 ===\n",
      "=== epoch[61/100], loss: 0.190419, valid_loss: 0.179042 ===\n",
      "=== epoch[62/100], loss: 0.190406, valid_loss: 0.179036 ===\n",
      "=== epoch[63/100], loss: 0.190406, valid_loss: 0.179035 ===\n",
      "=== epoch[64/100], loss: 0.190406, valid_loss: 0.179036 ===\n",
      "=== epoch[65/100], loss: 0.190406, valid_loss: 0.179035 ===\n",
      "=== epoch[66/100], loss: 0.190406, valid_loss: 0.179036 ===\n",
      "=== epoch[67/100], loss: 0.190406, valid_loss: 0.179036 ===\n",
      "=== epoch[68/100], loss: 0.190406, valid_loss: 0.179037 ===\n",
      "=== epoch[69/100], loss: 0.190406, valid_loss: 0.179036 ===\n",
      "=== epoch[70/100], loss: 0.190405, valid_loss: 0.179037 ===\n",
      "=== epoch[71/100], loss: 0.190399, valid_loss: 0.179035 ===\n",
      "=== epoch[72/100], loss: 0.190399, valid_loss: 0.179033 ===\n",
      "=== epoch[73/100], loss: 0.190399, valid_loss: 0.179032 ===\n",
      "=== epoch[74/100], loss: 0.190399, valid_loss: 0.179031 ===\n",
      "=== epoch[75/100], loss: 0.190398, valid_loss: 0.179030 ===\n",
      "=== epoch[76/100], loss: 0.190398, valid_loss: 0.179030 ===\n",
      "=== epoch[77/100], loss: 0.190398, valid_loss: 0.179029 ===\n",
      "=== epoch[78/100], loss: 0.190398, valid_loss: 0.179029 ===\n",
      "=== epoch[79/100], loss: 0.190398, valid_loss: 0.179029 ===\n",
      "=== epoch[80/100], loss: 0.190398, valid_loss: 0.179029 ===\n",
      "=== epoch[81/100], loss: 0.190397, valid_loss: 0.179029 ===\n",
      "=== epoch[82/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[83/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[84/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[85/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[86/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[87/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[88/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[89/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[90/100], loss: 0.190397, valid_loss: 0.179028 ===\n",
      "=== epoch[91/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[92/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[93/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[94/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[95/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[96/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[97/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[98/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[99/100], loss: 0.190396, valid_loss: 0.179028 ===\n",
      "=== epoch[100/100], loss: 0.190396, valid_loss: 0.179028 ===\n"
     ]
    }
   ],
   "source": [
    "# Build linear model\n",
    "feature_columns = ['travel_time', 'uplink_mean_tt', 'downlink_mean_tt']\n",
    "\n",
    "B     = 256\n",
    "D_in  = g_region_temporal * len(feature_columns)\n",
    "D_out = 1\n",
    "TRAIN_SET_RATIO = 0.8\n",
    "\n",
    "# Single linear layer\n",
    "model = torch.nn.Linear(D_in, D_out)\n",
    "model.cuda()\n",
    "\n",
    "def rmse(y_hat, y):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return torch.sqrt(torch.mean((y - y_hat).pow(2)))\n",
    "\n",
    "def mape(y_hat, y):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return torch.mean(((y - y_hat) / y).abs())\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "loss_fn = mape\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 我们需要按link_ID切分dataset，因为不同link的数据不能看作一个时间序列\n",
    "datasets_train = []\n",
    "datasets_valid = []\n",
    "link_no = ds_train_full.link_ID.unique().shape[0]\n",
    "counter = 0\n",
    "for link_ID, link_ds in ds_train_full.groupby('link_ID'):\n",
    "    counter += 1\n",
    "    if counter < link_no * TRAIN_SET_RATIO:\n",
    "        datasets_train.append(PandasDataset(link_ds, feature_columns))\n",
    "    else:\n",
    "        datasets_valid.append(PandasDataset(link_ds, feature_columns))\n",
    "\n",
    "print('train set size:', len(datasets_train) * len(datasets_train[0]))\n",
    "print('valid set size:', len(datasets_valid) * len(datasets_valid[0]))\n",
    "dataset_train = data.ConcatDataset(datasets_train)\n",
    "dataset_valid = data.ConcatDataset(datasets_valid)\n",
    "loader_train = data.DataLoader(dataset_train, batch_size=B, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "loader_valid = data.DataLoader(dataset_valid, batch_size=B, shuffle=False, num_workers=4, collate_fn=collate)\n",
    "\n",
    "def validate():\n",
    "    eval_running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batch in enumerate(loader_valid):\n",
    "        x_batch = Variable(sample_batch['feature']).cuda()\n",
    "        y_batch = Variable(sample_batch['label']).cuda()\n",
    "        y_batch_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        eval_running_loss += loss.data[0]\n",
    "        counter += 1\n",
    "        \n",
    "    return eval_running_loss / counter\n",
    "\n",
    "num_epochs = 100\n",
    "epoch_loss_records = []\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "for epoch in range(num_epochs):\n",
    "    lr_scheduler.step()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for i_batch, sample_batch in enumerate(loader_train):\n",
    "        x_batch = Variable(sample_batch['feature']).cuda()\n",
    "        y_batch = Variable(sample_batch['label']).cuda()\n",
    "\n",
    "        # forward\n",
    "        y_batch_pred = model(x_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        counter += 1\n",
    "\n",
    "    epoch_mean_loss = running_loss / counter\n",
    "    eval_mean_loss = validate()\n",
    "    print('=== epoch[{}/{}], loss: {:.6f}, valid_loss: {:.6f} ==='\n",
    "                  .format(epoch + 1, num_epochs, epoch_mean_loss, eval_mean_loss))\n",
    "    epoch_loss_records.append(epoch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
